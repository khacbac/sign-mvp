# Speech-to-Sign Language MVP â€” Progress Log

This document tracks **what has been completed so far** in the Speech-to-Sign Language MVP project.
It will be **updated incrementally** as new phases are implemented.

---

## ğŸ“Œ Project Overview

**Goal:**
Build a minimal viable product (MVP) that:

1. Accepts an uploaded audio file
2. Converts speech to text (ASR)
3. Converts text into sign-language-friendly gloss
4. Displays corresponding sign language visuals (videos)

**Current Scope:**

- Sign Language: **American Sign Language (ASL)**
- Sign assets: **Public ASL dictionary videos**
- Focus: correctness, clarity, and explainability (course project)

---

## ğŸ§­ Phase Breakdown

| Phase   | Name                      | Status       |
| ------- | ------------------------- | ------------ |
| Phase 0 | Environment Setup (Conda) | âœ… Completed |
| Phase 1 | Speech-to-Text (ASR)      | âœ… Completed |
| Phase 2 | Text â†’ Gloss              | â³ Pending   |
| Phase 3 | Sign Assets (ASL Videos)  | â³ Pending   |
| Phase 4 | Sign Rendering            | â³ Pending   |
| Phase 5 | Full Integration          | â³ Pending   |

---

## ğŸ”¹ Phase 0 â€” Environment Setup (Completed)

### Objective

Create a clean, reproducible Python environment using **Conda**.

### Key Decisions

- Python version pinned to **3.10** for stability
- Conda used for system-level dependencies
- Pip used for ML libraries

### Environment Creation

```bash
conda create -n sign_mvp_clean python=3.10 -y
conda activate sign_mvp_clean
```

### Dependencies Installed

```bash
conda install -c conda-forge ffmpeg opencv -y
pip install "numpy<2.4"
pip install torch torchvision torchaudio
pip install openai-whisper
```

### Environment Export

```bash
conda env export --from-history > environment.yml
```

This ensures:

- Minimal dependency list
- No environment bloat
- Easy reproduction on other machines

---

## ğŸ”¹ Phase 1 â€” Speech-to-Text (ASR) (Completed)

### Objective

Convert an input audio file into plain text using a pretrained model.

### Technology Used

- **OpenAI Whisper (base model)**
- No training or fine-tuning
- Offline inference

### Input / Output

**Input:**

- Audio file (e.g. `input/audio.wav`)

**Output:**

```text
I want to drink water
```

---

### Implementation File

```
asr/transcribe.py
```

### Key Implementation Notes

- Whisper model is loaded once per execution
- Audio path is passed via command-line argument
- Output text is printed to terminal

### macOS Compatibility Fix

To avoid OpenMP runtime conflicts:

```python
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
```

This line must appear **before importing Whisper**.

---

### How to Run

```bash
conda activate sign_mvp_clean
python asr/transcribe.py input/audio.wav
```

### Example Output

```text
[INFO] Loading Whisper model...
[INFO] Transcribing audio: input/audio.wav

=== TRANSCRIPTION RESULT ===
I want to drink water
```

---

### Conceptual Flow (Phase 1)

```
Audio File
   â†“
Feature Extraction (Mel Spectrogram)
   â†“
Whisper Transformer Model
   â†“
Plain Text Output
```

---

### Report-Ready Description

> Speech recognition is implemented using OpenAIâ€™s Whisper pretrained model. Given an uploaded audio file, the system performs offline inference to convert spoken language into text. The transcription output is returned as plain text and serves as input for subsequent text processing and sign language translation stages.

---

## ğŸ“ Current Project Structure

```
sign_mvp/
â”‚
â”œâ”€â”€ asr/
â”‚   â””â”€â”€ transcribe.py
â”œâ”€â”€ input/
â”‚   â””â”€â”€ audio.wav
â”œâ”€â”€ environment.yml
â””â”€â”€ README.md  (this file)
```

---

## ğŸ”¹ Phase 2 â€” Text â†’ Gloss (Rule-Based NLP) (Completed)

### Objective

Convert natural language text into **sign-language-friendly gloss tokens**.

This phase bridges **speech recognition (Phase 1)** and **sign rendering (later phases)**.

---

## ğŸ§  Design Rationale

Sign languages do not follow spoken-language grammar. Instead of translating word-for-word, we convert text into **gloss**:

- Uppercase keywords
- Simplified structure
- Removal of unnecessary function words

This MVP uses a **rule-based approach**:

- No machine learning
- Fully explainable
- Deterministic output

---

## ğŸ”¤ Input / Output

**Input (from Phase 1):**

```text
I want to drink water
```

**Output (Gloss Tokens):**

```text
[I, WANT, WATER]
```

---

## ğŸªœ Processing Steps

1. Convert text to lowercase
2. Remove punctuation
3. Tokenize text into words
4. Remove stopwords (e.g., "to", "the", "is", "drink")
5. Convert remaining words to uppercase

---

## ğŸ“ Implementation File

```
nlp/text_to_gloss.py
```

---

## ğŸ§© Stopword Strategy

Stopwords are words that:

- Do not contribute to sign meaning
- Are often omitted in sign language

### Example Stopwords

```
{"to", "the", "a", "an", "is", "are", "am", "drink", "do"}
```

This list is **manually defined** and can be easily extended.

---

## ğŸ§ª Testing Examples

| Input Sentence        | Gloss Output           |
| --------------------- | ---------------------- |
| I want water          | [I, WANT, WATER]       |
| Hello I want food     | [HELLO, I, WANT, FOOD] |
| I want to drink water | [I, WANT, WATER]       |

---

## ğŸ§  Conceptual Flow (Phase 2)

```
Text Output (ASR)
   â†“
Rule-Based Filtering
   â†“
Gloss Tokens
```

---

## ğŸ“ Report-Ready Description

> The text-to-gloss module converts transcribed speech into a simplified gloss representation suitable for sign language rendering. A rule-based approach is adopted to remove non-essential function words and normalize remaining keywords into uppercase gloss tokens. This design ensures full transparency and ease of explanation for an MVP system.

---

## ğŸ”¹ Phase 3 â€” Sign Assets (ASL Video Collection & Mapping) (Completed)

### Objective

Prepare **visual sign language assets** and map gloss tokens to corresponding ASL videos.

This phase enables the system to convert abstract gloss tokens into **concrete visual representations**.

---

## ğŸ§  Design Rationale

Instead of generating sign videos (which is complex and data-intensive), this MVP:

- Uses **pre-recorded ASL videos** from public educational dictionaries
- Maps each gloss token to a short video clip
- Plays videos sequentially in later phases

This approach:

- Avoids video synthesis
- Requires no model training
- Is reliable and easy to demonstrate

---

## ğŸ“¦ Sign Vocabulary (MVP)

For the MVP, a small but expressive vocabulary is sufficient.

### Selected Gloss Tokens

```
I
YOU
WANT
NEED
WATER
FOOD
HELLO
THANK
YES
NO
```

These words allow multiple meaningful demo sentences.

---

## ğŸ“ Asset Directory Structure

```
sign_mvp/
â”‚
â”œâ”€â”€ signs/
â”‚   â””â”€â”€ videos/
â”‚       â”œâ”€â”€ I.mp4
â”‚       â”œâ”€â”€ WANT.mp4
â”‚       â”œâ”€â”€ WATER.mp4
â”‚       â”œâ”€â”€ HELLO.mp4
â”‚       â””â”€â”€ ...
```

---

## ğŸ¥ Video Collection Guidelines

- Source: public ASL educational dictionaries (e.g. Lifeprint)
- Format: `.mp4`
- Duration: 1â€“3 seconds per sign
- Resolution: any (OpenCV will handle scaling)
- Naming: **UPPERCASE**, matches gloss exactly

---

## ğŸ—‚ Gloss-to-Video Mapping

### Mapping File

```
mapping/gloss_to_video.json
```

### Example Content

```json
{
  "I": "I.mp4",
  "YOU": "YOU.mp4",
  "WANT": "WANT.mp4",
  "WATER": "WATER.mp4",
  "FOOD": "FOOD.mp4",
  "HELLO": "HELLO.mp4",
  "THANK": "THANK.mp4"
}
```

This file decouples **language processing** from **visual rendering**.

---

## ğŸ§ª Verification Steps

1. Open each video manually to ensure it plays correctly
2. Confirm filenames exactly match gloss tokens
3. Validate JSON format (no trailing commas)

---

## ğŸ§  Conceptual Flow (Phase 3)

```
Gloss Tokens
   â†“
Gloss-to-Video Mapping
   â†“
ASL Video Assets
```

---

## ğŸ“ Report-Ready Description

> To visualize sign language output, the system uses pre-recorded ASL sign videos sourced from public educational dictionaries. Each gloss token is mapped to a corresponding video file using a simple lookup table. This design avoids the complexity of video generation while enabling a clear and effective demonstration of speech-to-sign translation.

---

## ğŸ”¹ Phase 4 â€” Sign Rendering (Video Playback) (Completed)

### Objective

Render sign language output by **playing ASL videos sequentially** according to the gloss order.

This phase converts symbolic gloss tokens into **visible sign language output**.

---

## ğŸ§  Design Rationale

Rather than synthesizing animations or avatars, the MVP:

- Uses OpenCV to play pre-recorded ASL videos
- Displays videos one after another in gloss order
- Keeps rendering logic simple and reliable

This approach is:

- Lightweight
- Cross-platform
- Easy to debug and explain

---

## ğŸ“ Implementation File

```
renderer/play_sign_sequence.py
```

---

## ğŸ”„ Rendering Logic

1. Load gloss-to-video mapping (`gloss_to_video.json`)
2. For each gloss token:

   - Find corresponding video file
   - Load video with OpenCV
   - Display frames sequentially

3. Close video window after playback

---

## ğŸ§© Key Functions

- `play_video(video_path)` â€” plays a single ASL video
- `play_gloss_sequence(gloss_list)` â€” plays a list of videos in order

---

## âš ï¸ Missing Gloss Handling

If a gloss token does not exist in the mapping:

- The system prints a warning
- Rendering continues with remaining glosses

This ensures robustness during demos.

---

## ğŸ§  Conceptual Flow (Phase 4)

```
Gloss Tokens
   â†“
Video Lookup
   â†“
OpenCV Playback
```

---

## ğŸ§ª Example Execution

Input Gloss:

```text
[I, WANT, WATER]
```

Visual Output:

```
I.mp4 â†’ WANT.mp4 â†’ WATER.mp4
```

---

## ğŸ“ Report-Ready Description

> The sign rendering module visualizes sign language output by sequentially playing pre-recorded ASL sign videos using OpenCV. Each gloss token is mapped to a corresponding video file, and videos are displayed in order to simulate continuous sign language communication.

---

## ğŸ”¹ Phase 5 â€” Full System Integration (Completed)

### Objective

Integrate all previous phases into a **single end-to-end pipeline**:

```
Audio â†’ ASR â†’ Text â†’ Gloss â†’ Sign Video Playback
```

The system can now be executed with **one command**, demonstrating a complete MVP.

---

## ğŸ§  Integration Strategy

Instead of tightly coupling modules, Phase 5:

- Imports each phase as an independent module
- Passes outputs between phases as plain Python objects
- Keeps each phase testable in isolation

This preserves modularity and simplifies debugging.

---

## ğŸ“ New File Added

```
run_pipeline.py
```

---

## ğŸ”„ End-to-End Flow

1. Load input audio file
2. Transcribe speech using Whisper (Phase 1)
3. Convert text to gloss tokens (Phase 2)
4. Map gloss tokens to sign videos (Phase 3)
5. Render sign language output using OpenCV (Phase 4)

---

## ğŸ§ª Example Execution

```bash
python run_pipeline.py input/audio.wav
```

Expected behavior:

- Console shows transcription and gloss output
- A window opens playing ASL videos sequentially

---

## ğŸ“ Report-Ready Description

> The complete system integrates speech recognition, rule-based language processing, and sign rendering into a unified pipeline. Given an audio input, the system automatically transcribes speech, converts it into sign gloss, and visualizes sign language output using pre-recorded ASL videos.

---

## ğŸ‰ Project Status

All MVP phases are completed:

- Phase 0: Environment Setup âœ…
- Phase 1: ASR (Whisper) âœ…
- Phase 2: Text â†’ Gloss Conversion âœ…
- Phase 3: Sign Asset Mapping âœ…
- Phase 4: Sign Rendering âœ…
- **Phase 5: Full Integration âœ…**

The project is now ready for demo, evaluation, and submission.
